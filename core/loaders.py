#   core/loaders.py
# ƒê√¢y l√† "th·ªß kho". Nhi·ªám v·ª• duy nh·∫•t l√† Load Model. Sau n√†y s·∫Ω th√™m h√†m load_lora, load_controlnet v√†o class n√†y c·ª±c k·ª≥ g·ªçn g√†ng.

import torch
from diffusers import AutoPipelineForText2Image, AutoPipelineForImage2Image
from .config import Config

class ModelLoader:
    def __init__(self):
        self.device = "cuda" if torch.cuda.is_available() else "cpu"
        self.pipeline = None
        self.current_type = None # "txt2img" ho·∫∑c "img2img"

    def load_base_pipeline(self, task_type="txt2img"):
        """Load ho·∫∑c chuy·ªÉn ƒë·ªïi pipeline gi·ªØa c√°c ch·∫ø ƒë·ªô"""
        
        # 1. Load m·ªõi n·∫øu ch∆∞a c√≥
        if self.pipeline is None:
            model_id = Config.get_model_path()
            print(f"üì• Loading Base Model from: {model_id}...")
            
            self.pipeline = AutoPipelineForText2Image.from_pretrained(
                model_id, 
                torch_dtype=torch.float16, 
                variant="fp16", 
                use_safetensors=True
            )
            # T·ªëi ∆∞u cho Colab T4
            self.pipeline.enable_model_cpu_offload()
            print("‚úÖ Model loaded.")

        # 2. Chuy·ªÉn ƒë·ªïi (Switching) m√† kh√¥ng load l·∫°i RAM
        if task_type == "img2img" and self.current_type != "img2img":
            self.pipeline = AutoPipelineForImage2Image.from_pipe(self.pipeline)
            self.current_type = "img2img"
            
        elif task_type == "txt2img" and self.current_type != "txt2img":
            self.pipeline = AutoPipelineForText2Image.from_pipe(self.pipeline)
            self.current_type = "txt2img"
            
        return self.pipeline

    # --- Sau n√†y th√™m t√≠nh nƒÉng ·ªü ƒë√¢y ---
    def load_lora_weights(self, lora_path):
        if self.pipeline:
            print(f"Loading LoRA from {lora_path}")
            self.pipeline.load_lora_weights(lora_path)
            
    def unload_lora(self):
        if self.pipeline:
            self.pipeline.unload_lora_weights()