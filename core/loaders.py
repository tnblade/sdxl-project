#   core/loaders.py
# ÄÃ¢y lÃ  "thá»§ kho". Nhiá»‡m vá»¥ duy nháº¥t lÃ  Load Model. Sau nÃ y sáº½ thÃªm hÃ m load_lora, load_controlnet vÃ o class nÃ y cá»±c ká»³ gá»n gÃ ng.

import torch
from diffusers import AutoPipelineForText2Image, AutoPipelineForImage2Image
from .config import Config

class ModelLoader:
    def __init__(self):
        self.device = "cuda" if torch.cuda.is_available() else "cpu"
        self.pipeline = None
        self.current_type = None # "txt2img" hoáº·c "img2img"

    def load_base_pipeline(self, task_type="txt2img"):
        """Load hoáº·c chuyá»ƒn Ä‘á»•i pipeline giá»¯a cÃ¡c cháº¿ Ä‘á»™"""
        
        # 1. Load má»›i náº¿u chÆ°a cÃ³
        if self.pipeline is None:
            model_id = Config.get_model_path()
            print(f"ğŸ“¥ Loading Base Model from: {model_id}...")
            
            self.pipeline = AutoPipelineForText2Image.from_pretrained(
                model_id, 
                torch_dtype=torch.float16, 
                variant="fp16", 
                use_safetensors=True
            )
            # Tá»‘i Æ°u cho Colab T4
            self.pipeline.enable_model_cpu_offload()
            print("âœ… Model loaded.")

        # 2. Chuyá»ƒn Ä‘á»•i (Switching) mÃ  khÃ´ng load láº¡i RAM
        if task_type == "img2img" and self.current_type != "img2img":
            self.pipeline = AutoPipelineForImage2Image.from_pipe(self.pipeline)
            self.current_type = "img2img"
            
        elif task_type == "txt2img" and self.current_type != "txt2img":
            self.pipeline = AutoPipelineForText2Image.from_pipe(self.pipeline)
            self.current_type = "txt2img"
            
        return self.pipeline

    # --- Sau nÃ y thÃªm tÃ­nh nÄƒng á»Ÿ Ä‘Ã¢y ---
    def load_lora_weights(self, lora_path):
        if self.pipeline:
            print(f"Loading LoRA from {lora_path}")
            self.pipeline.load_lora_weights(lora_path)
            
    def unload_lora(self):
        if self.pipeline:
            self.pipeline.unload_lora_weights()
            
    # --- THÃŠM 2 HÃ€M NÃ€Y CHO BENCHMARK ---
    def load_lora(self, lora_path, adapter_name="default"):
        """Náº¡p LoRA vÃ o pipeline Ä‘ang cháº¡y"""
        if self.pipeline:
            print(f"ğŸ”„ Loading LoRA adapter: {lora_path}")
            self.pipeline.load_lora_weights(lora_path, adapter_name=adapter_name)
            self.pipeline.fuse_lora() # Káº¿t há»£p weights Ä‘á»ƒ cháº¡y nhanh hÆ¡n
            print("âœ… LoRA Loaded & Fused.")

    def unload_lora(self):
        """Gá»¡ bá» LoRA Ä‘á»ƒ quay vá» model gá»‘c"""
        if self.pipeline:
            print(f"ğŸ”„ Unloading LoRA...")
            self.pipeline.unfuse_lora()
            self.pipeline.unload_lora_weights()
            print("âœ… LoRA Unloaded.")