# fine_tuning/lora.py
# Script kh·ªüi ch·∫°y qu√° tr√¨nh fine-tuning LoRA cho SDXL 
# S·ª≠ d·ª•ng script chu·∫©n t·ª´ th∆∞ vi·ªán Diffusers c·ªßa HuggingFace v·ªõi m·ªôt s·ªë c·∫•u h√¨nh t·ªëi ∆∞u cho T4 


import os
import subprocess
import argparse
import sys
import torch
from accelerate.utils import write_basic_config

# --- Hack ƒë∆∞·ªùng d·∫´n ƒë·ªÉ import Config ---
sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), '..')))
from core.config import Config

SCRIPT_URL = "https://raw.githubusercontent.com/huggingface/diffusers/main/examples/text_to_image/train_text_to_image_lora_sdxl.py"
SCRIPT_NAME = "train_lora_sdxl_script.py"

def download_script():
    """T·∫£i script training chu·∫©n"""
    if not os.path.exists(SCRIPT_NAME):
        print(f"‚è≥ [LoRA] ƒêang t·∫£i script chu·∫©n t·ª´ HuggingFace...")
        try:
            subprocess.run(["wget", "-q", SCRIPT_URL, "-O", SCRIPT_NAME], check=True)
        except Exception as e:
            print(f"‚ùå L·ªói t·∫£i script: {e}")
            sys.exit(1)

def run_lora_training(data_dir, output_dir, prompt, base_model_path):
    if output_dir is None:
        output_dir = "output_lora_result"

    # --- 1. X·ª¨ L√ù MODEL PATH (FIX L·ªñI DEVICE MISMATCH) ---
    # Script training chu·∫©n KH√îNG h·ªó tr·ª£ file .safetensors ƒë∆°n l·∫ª t·ªët.
    # N·∫øu ph√°t hi·ªán input l√† file ƒë∆°n, ta bu·ªôc ph·∫£i d√πng repo g·ªëc tr√™n HuggingFace
    # ƒë·ªÉ ƒë·∫£m b·∫£o script t·∫£i ƒë√∫ng c·∫•u tr√∫c th∆∞ m·ª•c (UNet/VAE/TextEncoder) v·ªÅ GPU.
    if base_model_path.endswith(".safetensors"):
        print(f"‚ö†Ô∏è C·∫¢NH B√ÅO: Script training kh√¥ng h·ªó tr·ª£ tr·ª±c ti·∫øp file ƒë∆°n (.safetensors).")
        print(f"üîÑ ƒêang chuy·ªÉn sang d√πng Repo g·ªëc: stabilityai/stable-diffusion-xl-base-1.0")
        print(f"   (Vi·ªác n√†y gi√∫p tr√°nh l·ªói 'Expected all tensors to be on the same device')")
        train_model_path = "stabilityai/stable-diffusion-xl-base-1.0"
    else:
        train_model_path = base_model_path

    # --- 2. C·∫§U H√åNH MULTI-GPU (T·ª∞ ƒê·ªòNG) ---
    # Ki·ªÉm tra s·ªë l∆∞·ª£ng GPU
    gpu_count = torch.cuda.device_count()
    print(f"üöÄ Ph√°t hi·ªán {gpu_count} GPU.")
    
    # T·∫°o config m·∫∑c ƒë·ªãnh cho accelerate (tr√°nh l·ªói ch∆∞a config)
    write_basic_config(mixed_precision="fp16")

    cmd = ["accelerate", "launch"]

    # N·∫øu c√≥ nhi·ªÅu GPU, th√™m tham s·ªë ƒë·ªÉ ch·∫°y song song (Nhanh g·∫•p ƒë√¥i)
    if gpu_count > 1:
        print("üî• K√≠ch ho·∫°t ch·∫ø ƒë·ªô Multi-GPU Training!")
        cmd.extend([
            "--multi_gpu",
            f"--num_processes={gpu_count}"
        ])

    # Th√™m script v√† c√°c tham s·ªë
    cmd.append(SCRIPT_NAME)
    
    # C√°c tham s·ªë training t·ªëi ∆∞u
    args = [
        f"--pretrained_model_name_or_path={train_model_path}",
        f"--train_data_dir={data_dir}",
        "--caption_column=text",
        "--resolution=1024",
        "--random_flip",
        "--train_batch_size=1",
        "--num_train_epochs=10",
        "--checkpointing_steps=500",
        "--learning_rate=1e-4",
        "--lr_scheduler=constant",
        "--lr_warmup_steps=0",
        "--mixed_precision=fp16",
        "--seed=42",
        f"--output_dir={output_dir}",
        f"--validation_prompt={prompt}",
        
        # --- C√ÅC THAM S·ªê T·ªêI ∆ØU B·ªò NH·ªö QUAN TR·ªåNG ---
        "--gradient_checkpointing", # Ti·∫øt ki·ªám VRAM
        "--use_8bit_adam",          # Optimizer nh·∫π
        "--report_to=tensorboard",
        "--logging_dir=logs"
    ]
    
    # N·∫øu d√πng repo HF, ta c·∫ßn preload model v√†o cache ƒë·ªÉ tr√°nh l·ªói timeout khi ch·∫°y multi-process
    # Nh∆∞ng accelerate usually handles this.
    
    cmd.extend(args)

    print(f"\nexecuting command: {' '.join(cmd)}")
    print(f"üìÇ Model Training: {train_model_path}")
    
    try:
        subprocess.run(cmd, check=True)
        print(f"\n‚úÖ [LoRA] Training ho√†n t·∫•t! File t·∫°i: {output_dir}/pytorch_lora_weights.safetensors")
    except subprocess.CalledProcessError as e:
        print(f"\n‚ùå [LoRA] L·ªói trong qu√° tr√¨nh train. H√£y ki·ªÉm tra log ph√≠a tr√™n.")

if __name__ == "__main__":
    parser = argparse.ArgumentParser()
    parser.add_argument("--data_dir", type=str, required=True, help="Folder ·∫£nh train")
    parser.add_argument("--prompt", type=str, required=True, help="Prompt k√≠ch ho·∫°t")
    parser.add_argument("--output_dir", type=str, default=None, help="Folder l∆∞u k·∫øt qu·∫£")
    parser.add_argument("--base_model", type=str, default=None, help="ƒê∆∞·ªùng d·∫´n Base Model")
    
    args = parser.parse_args()
    
    os.chdir(os.path.dirname(os.path.abspath(__file__)))
    download_script()
    
    # L·∫•y path t·ª´ Config n·∫øu kh√¥ng truy·ªÅn v√†o
    if args.base_model:
        final_model_path = args.base_model
    else:
        final_model_path = Config.get_model_path()
    
    run_lora_training(args.data_dir, args.output_dir, args.prompt, final_model_path)