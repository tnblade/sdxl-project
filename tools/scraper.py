# CÃ´ng cá»¥ táº£i áº£nh tá»« Safebooru dá»±a trÃªn tá»« khÃ³a
# tools/scraper.py


import os
import requests
import argparse
from tqdm import tqdm
import time

# API cá»§a Safebooru (An toÃ n, khÃ´ng cáº§n key, chuyÃªn Anime)
API_URL = "https://safebooru.org/index.php"

def download_images(tags, limit, output_dir):
    if not os.path.exists(output_dir):
        os.makedirs(output_dir)
        
    print(f"ğŸ” Äang tÃ¬m kiáº¿m: '{tags}' | Sá»‘ lÆ°á»£ng: {limit}...")
    
    count = 0
    page = 0
    
    # Headers Ä‘á»ƒ giáº£ láº­p trÃ¬nh duyá»‡t (trÃ¡nh bá»‹ cháº·n)
    headers = {'User-Agent': 'Mozilla/5.0'}

    pbar = tqdm(total=limit, desc="Downloading")
    
    while count < limit:
        # Gá»i API láº¥y danh sÃ¡ch áº£nh (XML/JSON)
        params = {
            "page": "dapi",
            "s": "post",
            "q": "index",
            "json": 1,
            "limit": 100, # Láº¥y 100 áº£nh má»—i trang
            "pid": page,
            "tags": tags
        }
        
        try:
            response = requests.get(API_URL, params=params, headers=headers)
            if response.status_code != 200:
                print(f"âŒ Lá»—i káº¿t ná»‘i: {response.status_code}")
                break
                
            posts = response.json()
            if not posts:
                print("âš ï¸ Háº¿t áº£nh Ä‘á»ƒ táº£i!")
                break
                
            for post in posts:
                if count >= limit: break
                
                # Bá» qua áº£nh khÃ´ng cÃ³ URL
                if 'file_url' not in post: continue
                
                img_url = "https://safebooru.org/images/" + post['directory'] + "/" + post['image']
                
                # TÃªn file
                filename = f"{post['id']}.jpg"
                filepath = os.path.join(output_dir, filename)
                
                # Táº£i áº£nh
                if not os.path.exists(filepath):
                    img_data = requests.get(img_url, headers=headers).content
                    with open(filepath, 'wb') as f:
                        f.write(img_data)
                    
                    count += 1
                    pbar.update(1)
                    
            page += 1
            time.sleep(1) # Nghá»‰ 1 chÃºt Ä‘á»ƒ khÃ´ng bá»‹ server cháº·n
            
        except Exception as e:
            print(f"âŒ Lá»—i: {e}")
            break

    pbar.close()
    print(f"\nâœ… ÄÃ£ táº£i xong {count} áº£nh vÃ o thÆ° má»¥c: {output_dir}")

if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="Anime/Genshin Image Scraper")
    parser.add_argument("--tags", type=str, required=True, help="Tá»« khÃ³a (VD: genshin_impact, 1girl, solo)")
    parser.add_argument("--limit", type=int, default=20, help="Sá»‘ lÆ°á»£ng áº£nh cáº§n táº£i")
    parser.add_argument("--output", type=str, default="raw_images", help="ThÆ° má»¥c lÆ°u")
    
    args = parser.parse_args()
    download_images(args.tags, args.limit, args.output)